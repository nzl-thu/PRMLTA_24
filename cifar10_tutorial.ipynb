{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nzl-thu/PRMLTA/blob/main/cifar10_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rZkONWgngm0h"
   },
   "source": [
    "\n",
    "# Tensors\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays\n",
    "and matrices. In PyTorch, we use tensors to encode the inputs and\n",
    "outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on\n",
    "GPUs or other specialized hardware to accelerate computing. If you’re familiar with ndarrays, you’ll\n",
    "be right at home with the Tensor API. If not, follow along in this quick\n",
    "API walkthrough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wWIJTkAsgm0h"
   },
   "source": [
    "## Tensor Initialization\n",
    "\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:\n",
    "\n",
    "**Directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is automatically inferred.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "91gvRAiOgm0h"
   },
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa - see `bridge-to-np-label`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nfVKHBTjgm0i"
   },
   "source": [
    "**From another tensor:**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tGVa2GI7gm0i"
   },
   "source": [
    "**With random or constant values:**\n",
    "\n",
    "``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "S1oq-v3bgm0i"
   },
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GiU5qG11gm0i"
   },
   "source": [
    "## Tensor Attributes\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ML5jRwTNgm0i"
   },
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hTJc_VhTgm0i"
   },
   "source": [
    "## Tensor Operations\n",
    "\n",
    "Over 100 tensor operations, including transposing, indexing, slicing,\n",
    "mathematical operations, linear algebra, random sampling, and more are\n",
    "comprehensively described\n",
    "[here](https://pytorch.org/docs/stable/torch.html)_.\n",
    "\n",
    "Each of them can be run on the GPU (at typically higher speeds than on a\n",
    "CPU). If you’re using Colab, allocate a GPU by going to Edit > Notebook\n",
    "Settings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "E1PbRYTmgm0i"
   },
   "source": [
    "Try out some of the operations from the list.\n",
    "If you're familiar with the NumPy API, you'll find the Tensor API a breeze to use.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TJfFOe7qgm0j"
   },
   "source": [
    "**Standard numpy-like indexing and slicing:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OZST5H2Cgm0j"
   },
   "source": [
    "**Joining tensors** You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension.\n",
    "See also [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)_,\n",
    "another tensor joining op that is subtly different from ``torch.cat``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "E2k-NUvygm0j"
   },
   "source": [
    "**Multiplying tensors**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "D7apA-tWgm0j"
   },
   "source": [
    "This computes the matrix multiplication between two tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fYJHoT7Zgm0j"
   },
   "source": [
    "**In-place operations**\n",
    "Operations that have a ``_`` suffix are in-place. For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "25Z5UsqPgm0j"
   },
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss\n",
    "     of history. Hence, their use is discouraged.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pmmdXqRXgm0j"
   },
   "source": [
    "# A toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KUxSwl4Sgm0j"
   },
   "source": [
    "In lecture 9, we have presented a simple demo with numpy for classification. Now we show how this can be achieved by pytorch.\n",
    "\n",
    "First, let's prepare our toy dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize weights and data\n",
    "N, Din, H, Dout = 128, 64, 64, 1\n",
    "lr = 5e-5\n",
    "\n",
    "# Training data\n",
    "x_train = torch.randn(N, Din)\n",
    "y_train = torch.zeros(N, 1)\n",
    "x_train[:N // 2, 0] = x_train[:N // 2, 0] * 2 + 5\n",
    "y_train[:N // 2, 0] = 1\n",
    "\n",
    "# Test data\n",
    "N_test = N\n",
    "x_test = torch.randn(N_test, Din)\n",
    "y_test = torch.zeros(N_test, 1)\n",
    "x_test[:N_test // 2, 0] = x_test[:N_test // 2, 0] * 2 + 5\n",
    "y_test[:N_test // 2, 0] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BA8Z90jggm0j"
   },
   "source": [
    "Now let's visualize our training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train[:, 0], cmap=plt.cm.Spectral)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "S5zaNNaAgm0j"
   },
   "source": [
    "Now, we define our neural network with one hidden layer (with the dimension being `H`). The neural network is parameterized by two matrices: `theta1` (project input data into hidden space) and `theta2` (project from hidden space to output space):"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "theta1 = torch.randn(Din, H, requires_grad=True)\n",
    "theta2 = torch.randn(H, Dout, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BH7aEAABgm0j"
   },
   "source": [
    "## Forward Propagation and Loss Calculation\n",
    "We can easily use forward propagation to obtain the model's prediction and calculate the cross-entropy loss within few lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "h = torch.sigmoid(x_train.matmul(theta1))\n",
    "h.retain_grad()\n",
    "y_pred = torch.sigmoid(h.matmul(theta2))\n",
    "\n",
    "# Calculate loss\n",
    "loss = torch.mean(-(1 - y_train) * torch.log(1 - y_pred) - y_train * torch.log(y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ARJ_bmdSgm0j"
   },
   "source": [
    "## Back Propagation\n",
    "For gradient calculation, pytorch can automatically achieve this by a single call of `loss.backward()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "loss.backward()\n",
    "\n",
    "# obtain the gradients on theta1 and theta2\n",
    "print('dJ / d theta1=\\n', theta1.grad, \"\\nshape of dJ / d theta1:\", theta1.grad.shape)\n",
    "print('dJ / d theta2=\\n', theta2.grad, \"\\nshape of dJ / d theta2:\", theta2.grad.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fohNiFYvgm0k"
   },
   "source": [
    "We can also inspect the gradient $dJ/dh$, that is the gradient with respect to the activation of the hidden layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "print(\"dJ/dh=\\n\", h.grad)\n",
    "print(\"shape of dJ/dh:\", h.grad.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's test if the gradient calculated by pytorch is the same as our analytical solution. We take $\\theta_2$ as an example:\n",
    "$$\n",
    "\\frac{dJ}{d\\theta_2} = \\frac {h^T (y_{\\text{pred}}-y_{\\text{train}})}{N}\n",
    "$$\n",
    "Note that here $h$ and $y$ are all matrices since we are performing training on batch-level."
   ],
   "metadata": {
    "id": "XUyqi_29guTe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "print('dJ / d theta2 (Pytorch)=\\n', theta2.grad.squeeze())\n",
    "print('dJ / d theta2 (analytical)=\\n', (h.T @ (y_pred - y_train) / N).squeeze())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_-Qp2LyUgm0k"
   },
   "source": [
    "## Gradient Descent\n",
    "Next, we can use pytorch's optimizer to perform gradient descent easily:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD([theta1, theta2], lr=0.01)\n",
    "\n",
    "theta1_orig = theta1.clone().detach()  # record the parameters before gradient descent, we use theta_1 as an example\n",
    "optimizer.step()  # perform gradient descent\n",
    "print('theta_1 before gradient descent:\\n', theta1_orig)\n",
    "print('theta_1 after gradient descent:\\n', theta1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "VG8t2Dqugm0k"
   },
   "source": [
    "The parameters have already been updated by `optimizer.step`.\n",
    "We can verify this gradient descent step is equal as our learned formulation:\n",
    "\n",
    "$$\n",
    "\\theta_1\\leftarrow \\theta_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial \\theta_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "theta1_new_analytical = theta1_orig - 0.01 * theta1.grad\n",
    "print('theta_1 after gradient descent (analytical):\\n', theta1_new_analytical)\n",
    "print('theta_1 after gradient descent (analytical) - theta_1 after gradient descent (PyTorch):\\n', theta1_new_analytical - theta1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7-sFLCaogm0k"
   },
   "source": [
    "## Training Loop\n",
    "Now, let's perform training on this toy dataset by repeating the above forward & backward & gradient descent pipeline for 10000 times and see how the neural network learns. "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "for t in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward\n",
    "    h = torch.sigmoid(x_train.matmul(theta1))\n",
    "    y_pred = torch.sigmoid(h.matmul(theta2))\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = torch.mean(-(1 - y_train) * torch.log(1 - y_pred) - y_train * torch.log(y_pred))\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    # Test accuracy\n",
    "    with torch.no_grad():\n",
    "        h_test = torch.sigmoid(x_test.matmul(theta1))\n",
    "        y_pred_test = torch.sigmoid(h_test.matmul(theta2))\n",
    "        accuracy = torch.mean(((y_pred_test > 0.5) == y_test).float()).item() * 100.0\n",
    "        accuracy_list.append(accuracy)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights with gradient descent\n",
    "    optimizer.step()\n",
    "    \n",
    "# Plot\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel(\"training steps\")\n",
    "ax1.set_ylabel(\"training loss\")\n",
    "loss_curve = ax1.plot(range(len(loss_list)), loss_list, label=\"training loss\", color=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"test accuracy (%)\")\n",
    "accuracy_curve = ax2.plot(range(len(accuracy_list)), accuracy_list, label=\"test accuracy\", color=\"tab:orange\")\n",
    "\n",
    "curves = loss_curve + accuracy_curve\n",
    "labels = [c.get_label() for c in curves]\n",
    "ax1.legend(curves, labels, loc=\"center right\")\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pFQ55fI9gm0k"
   },
   "source": [
    "\n",
    "# Training a Classifier\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Generally, when you have to deal with image, text, audio or video data,\n",
    "you can use standard python packages that load data into a numpy array.\n",
    "Then you can convert this array into a ``torch.*Tensor``.\n",
    "\n",
    "Specifically for vision, we have created a package called\n",
    "``torchvision``, that has data loaders for common datasets such as\n",
    "ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz.,\n",
    "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
    "\n",
    "This provides a huge convenience and avoids writing boilerplate code.\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset.\n",
    "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
    "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of\n",
    "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "## Training an image classifier\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "1. Load and normalize the CIFAR10 training and test datasets using\n",
    "   ``torchvision``\n",
    "2. Define a Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data\n",
    "\n",
    "### 1. Load and normalize CIFAR10\n",
    "\n",
    "Using ``torchvision``, it’s extremely easy to load CIFAR10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "epsgNZk6gm0k"
   },
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "We transform them to Tensors of normalized range [-1, 1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1wGAhU82gm0k"
   },
   "source": [
    "Let us show some of the training images, for fun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6FPFN7iMgm0l"
   },
   "source": [
    "### 2. Define a Neural Network\n",
    "Here, we define a simple neural network with 2 hidden layers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3*32*32, 768)\n",
    "        self.fc2 = nn.Linear(768, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net().cuda()  # put the neural network to GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SpWdRFV5gm0l"
   },
   "source": [
    "### 3. Define a Loss function and optimizer\n",
    "Let's use a Classification Cross-Entropy loss.\n",
    "For optimizer, we adopt a highly effective one called Adam. \n",
    "\n",
    "Tips: If you are not sure which optimizer to use, try Adam first!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "k9olMCi8gm0l"
   },
   "source": [
    "### 4. Train the network\n",
    "\n",
    "This is when things start to get interesting.\n",
    "We simply have to loop over our data iterator, and feed the inputs to the\n",
    "network and optimize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()  # Note that we should also put data and label to GPU to accelerate training\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "RrDurkMvgm0l"
   },
   "source": [
    "Let's quickly save our trained model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JYqGTp8Bgm0l"
   },
   "source": [
    "See [here](https://pytorch.org/docs/stable/notes/serialization.html)\n",
    "for more details on saving PyTorch models.\n",
    "\n",
    "### 5. Test the network on the test data\n",
    "\n",
    "We have trained the network for 2 passes over the training dataset.\n",
    "But we need to check if the network has learnt anything at all.\n",
    "\n",
    "We will check this by predicting the class label that the neural network\n",
    "outputs, and checking it against the ground-truth. If the prediction is\n",
    "correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Okay, first step. Let us display images from the test set to get familiar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OKBHLhqcgm0l"
   },
   "source": [
    "Next, let's load back in our saved model (note: saving and re-loading the model\n",
    "wasn't necessary here, we only did it to illustrate how to do so):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "stQLKI6egm0l"
   },
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "outputs = net(images)\n",
    "print(\"outputs.shape:\", outputs.shape)\n",
    "print(\"outputs:\\n\", outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FMHriZjggm0l"
   },
   "source": [
    "The outputs are energies for the 10 classes.\n",
    "The higher the energy for a class, the more the network\n",
    "thinks that the image is of the particular class.\n",
    "So, let's get the index of the highest energy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gr1GE-r_gm0l"
   },
   "source": [
    "The results seem ok.\n",
    "\n",
    "Let us look at how the network performs on the whole dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0], data[1]\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7YnB_XPugm0l"
   },
   "source": [
    "That looks **way better than chance**, which is 10% accuracy (randomly picking\n",
    "a class out of 10 classes).\n",
    "That means the network have already learnt something with this small period of training.\n",
    "\n",
    "Hmmm, what are the classes that performed well, and the classes that did\n",
    "not perform well:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Gvh8EXt-gm0l"
   },
   "source": [
    "Okay, so what next?\n",
    "\n",
    "## Exercise: Build a more powerful neural network\n",
    "\n",
    "The previous example is a basic proof of concept of how we can use pytorch to build neural networks. Now it's your turn to build a more powerful neural network!\n",
    "\n",
    "### ResNet\n",
    "ResNet is a deep convolutional neural network, uses residual connections to address vanishing gradients and performance degradation in deep networks.\n",
    "\n",
    "Simply put, the core idea of ResNet is a so called \"skip\" connection:\n",
    "\n",
    "\\begin{align}\\text{without skip}&: y=f(x) \\\\ \\text{with skip}&: y=f(x)+x\\end{align}\n",
    "\n",
    "A ResNet is typically consists of 3 parts: \n",
    "1. The first part uses one convolutional layer to project the input images into a feature space.\n",
    "2. The second part consists of multiple blocks for feature processing.\n",
    "3. The third part performs a spatial pooling and converts the feature into classification results.\n",
    "The building block of the second part is the key of ResNet, which is achieved by BasicBlock class.\n",
    "\n",
    "### Your task: \n",
    "Finish the BasicBlock class and perform training & evaluation using your ResNet. Compare how it performs with the previous vannila neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(dim)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += x\n",
    "        out = F.relu(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            BasicBlock(dim=32) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.linear = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.blocks(out)\n",
    "        out = F.avg_pool2d(out, 32)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "net = ResNet(n_blocks=6).cuda()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xRq47KF0gm0m"
   },
   "source": [
    "Then, it's time to train & test the performance of your neural network! "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].cuda(), data[1].cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "B-ciZFXKgm0m"
   },
   "source": [
    "### Your task 2: \n",
    "Load CIFAR100 dataset in torchvision by yourself, then perform train & eval on CIFAR100 dataset.\n",
    "\n",
    "**Note:**The network and optimizer should be re-initialized before training on CIFAR100. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gWgzT23mgm0m"
   },
   "source": [
    "\n",
    "# A Gentle Introduction to ``torch.autograd``\n",
    "\n",
    "``torch.autograd`` is PyTorch’s automatic differentiation engine that powers\n",
    "neural network training. In this section, you will get a basic sense of how ``autograd`` collects gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fwpmS_Uhgm0m"
   },
   "source": [
    "In homework 8, you have encountered some matrix calculus equations. For example:\n",
    "\n",
    "\\begin{align}&\\frac{\\partial}{\\partial{X}}\\mathrm{\\text{Tr}}((Y-CX)^T(Y-CX))=-2 C^T (Y-CX)\\end{align}\n",
    "\n",
    "Now, let's see how we can perform this gradient calculation using pytorch's autograd mechanics (**without** the need of knowing the explicit formula of the gradient).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cLZN_fj3gm0m"
   },
   "source": [
    "We first create the matrix ``X``, ``C`` and ``Y``. Note that we pass ``requires_grad=True`` when creating matrix ``X``. This signals to ``autograd`` that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "X = torch.randn(4, 4, requires_grad=True)\n",
    "C = torch.randn(4, 4)\n",
    "Y = torch.randn(4, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "print('expected gradient on X:\\n', -2 * C.T @ (Y - C @ X))\n",
    "# calculate the result\n",
    "Q=torch.trace((Y-C @ X).T @ (Y-C.matmul(X)))\n",
    "# perform back-propagation\n",
    "Q.backward()\n",
    "print('autograd calculated gradient on X:\\n', X.grad)  # the gradients on X are stored in X.grad "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "WssHr9nNgm0m"
   },
   "source": [
    "Nice! The gradient calculated by autograd is the same as our analytical solution!\n",
    "\n",
    "Now, let's re-execute the cell and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "print('expected gradient on X:\\n', -2 * C.T @ (Y - C @ X))\n",
    "# calculate the result\n",
    "Q=torch.trace((Y-C @ X).T @ (Y-C.matmul(X)))\n",
    "# perform back-propagation\n",
    "Q.backward()\n",
    "print('autograd calculated gradient on X:\\n', X.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dmKOvEcogm0m"
   },
   "source": [
    "Why in this time the autograd calculated gradient is not the same as our analytical result? This is because every time ``backward()`` function is called, gradients calculated by autograd are **accumulated** into ``X.grad``. \n",
    "\n",
    "As a result, if we are only interested in the gradients for current operations, we must first clear previously stored gradients:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "print('expected gradient on X:\\n', -2 * C.T @ (Y - C @ X))\n",
    "Q=torch.trace((Y-C @ X).T @ (Y-C.matmul(X)))\n",
    "X.grad.zero_()  # Important! Clear the previously stored gradients\n",
    "Q.backward()\n",
    "print('autograd calculated gradient on X:\\n', X.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0BMvkW-3gm0m"
   },
   "source": [
    "Works as we expected again!\n",
    "\n",
    "Therefore, when performing neural network training, we will use ``zero_grad`` of optimizer to clear the gradient before performing gradient update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9U2yUXeRgm0m"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD([theta1, theta2], lr=0.01)\n",
    "\n",
    "theta1_orig = theta1.clone().detach()  # record the parameters before gradient descent, we use theta_1 as an example\n",
    "optimizer.step()  # perform gradient descent\n",
    "print('theta_1 before gradient descent:\\n', theta1_orig)\n",
    "print('theta_1 after gradient descent:\\n', theta1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjsSmKN2gm0n"
   },
   "source": [
    "The parameters have already been updated by `optimizer.step`.\n",
    "We can verify this gradient descent step is equal as our learned formulation:\n",
    "\n",
    "$$\n",
    "\\theta_1\\leftarrow \\theta_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial \\theta_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "theta1_new_analytical = theta1_orig - 0.01 * theta1.grad\n",
    "print('theta_1 after gradient descent (analytical):\\n', theta1_new_analytical)\n",
    "print('theta_1 after gradient descent (analytical) - theta_1 after gradient descent (PyTorch):\\n', theta1_new_analytical - theta1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1SqWVsIJ3UX"
   },
   "source": [
    "## Training Loop\n",
    "Now, let's perform training on this toy dataset by repeating the above forward & backward & gradient descent pipeline for 10000 times and see how the neural network learns. "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "for t in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward\n",
    "    h = torch.sigmoid(x_train.matmul(theta1))\n",
    "    y_pred = torch.sigmoid(h.matmul(theta2))\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = torch.mean(-(1 - y_train) * torch.log(1 - y_pred) - y_train * torch.log(y_pred))\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    # Test accuracy\n",
    "    with torch.no_grad():\n",
    "        h_test = torch.sigmoid(x_test.matmul(theta1))\n",
    "        y_pred_test = torch.sigmoid(h_test.matmul(theta2))\n",
    "        accuracy = torch.mean(((y_pred_test > 0.5) == y_test).float()).item() * 100.0\n",
    "        accuracy_list.append(accuracy)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights with gradient descent\n",
    "    optimizer.step()\n",
    "    \n",
    "# Plot\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel(\"training steps\")\n",
    "ax1.set_ylabel(\"training loss\")\n",
    "loss_curve = ax1.plot(range(len(loss_list)), loss_list, label=\"training loss\", color=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"test accuracy (%)\")\n",
    "accuracy_curve = ax2.plot(range(len(accuracy_list)), accuracy_list, label=\"test accuracy\", color=\"tab:orange\")\n",
    "\n",
    "curves = loss_curve + accuracy_curve\n",
    "labels = [c.get_label() for c in curves]\n",
    "ax1.legend(curves, labels, loc=\"center right\")\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4f3bSAA1eyC"
   },
   "source": [
    "\n",
    "# Training a Classifier\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Generally, when you have to deal with image, text, audio or video data,\n",
    "you can use standard python packages that load data into a numpy array.\n",
    "Then you can convert this array into a ``torch.*Tensor``.\n",
    "\n",
    "Specifically for vision, we have created a package called\n",
    "``torchvision``, that has data loaders for common datasets such as\n",
    "ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz.,\n",
    "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
    "\n",
    "This provides a huge convenience and avoids writing boilerplate code.\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset.\n",
    "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
    "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of\n",
    "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "## Training an image classifier\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "1. Load and normalize the CIFAR10 training and test datasets using\n",
    "   ``torchvision``\n",
    "2. Define a Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data\n",
    "\n",
    "### 1. Load and normalize CIFAR10\n",
    "\n",
    "Using ``torchvision``, it’s extremely easy to load CIFAR10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTSsPfYI1eyE"
   },
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "We transform them to Tensors of normalized range [-1, 1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vPiZ9NW1eyE"
   },
   "source": [
    "Let us show some of the training images, for fun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbPnPmIC1eyE"
   },
   "source": [
    "### 2. Define a Neural Network\n",
    "Here, we define a simple neural network with 2 hidden layers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3*32*32, 768)\n",
    "        self.fc2 = nn.Linear(768, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net().cuda()  # put the neural network to GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIxm3TlP1eyF"
   },
   "source": [
    "### 3. Define a Loss function and optimizer\n",
    "Let's use a Classification Cross-Entropy loss.\n",
    "For optimizer, we adopt a highly effective one called Adam. \n",
    "\n",
    "Tips: If you are not sure which optimizer to use, try Adam first!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiyR-gC01eyF"
   },
   "source": [
    "### 4. Train the network\n",
    "\n",
    "This is when things start to get interesting.\n",
    "We simply have to loop over our data iterator, and feed the inputs to the\n",
    "network and optimize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()  # Note that we should also put data and label to GPU to accelerate training\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 20 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f}')\n",
    "            running_loss = 0.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3koUhJxY1eyF"
   },
   "source": [
    "Let's quickly save our trained model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Almk2xs1eyF"
   },
   "source": [
    "See [here](https://pytorch.org/docs/stable/notes/serialization.html)\n",
    "for more details on saving PyTorch models.\n",
    "\n",
    "### 5. Test the network on the test data\n",
    "\n",
    "We have trained the network for 2 passes over the training dataset.\n",
    "But we need to check if the network has learnt anything at all.\n",
    "\n",
    "We will check this by predicting the class label that the neural network\n",
    "outputs, and checking it against the ground-truth. If the prediction is\n",
    "correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Okay, first step. Let us display images from the test set to get familiar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZBKiGVO1eyF"
   },
   "source": [
    "Next, let's load back in our saved model (note: saving and re-loading the model\n",
    "wasn't necessary here, we only did it to illustrate how to do so):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkgSw_gb1eyG"
   },
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "outputs = net(images)\n",
    "print(\"outputs.shape:\", outputs.shape)\n",
    "print(\"outputs:\\n\", outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSmu0_aZ1eyG"
   },
   "source": [
    "The outputs are energies for the 10 classes.\n",
    "The higher the energy for a class, the more the network\n",
    "thinks that the image is of the particular class.\n",
    "So, let's get the index of the highest energy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFKvbb131eyG"
   },
   "source": [
    "The results seem ok.\n",
    "\n",
    "Let us look at how the network performs on the whole dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0], data[1]\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcQNK_Lr1eyG"
   },
   "source": [
    "That looks **way better than chance**, which is 10% accuracy (randomly picking\n",
    "a class out of 10 classes).\n",
    "That means the network have already learnt something with this small period of training.\n",
    "\n",
    "Hmmm, what are the classes that performed well, and the classes that did\n",
    "not perform well:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf-9-yZt1eyG"
   },
   "source": [
    "Okay, so what next?\n",
    "\n",
    "## Exercise: Build a more powerful neural network\n",
    "\n",
    "The previous example is a basic proof of concept of how we can use pytorch to build neural networks. Now it's your turn to build a more powerful neural network!\n",
    "\n",
    "### ResNet\n",
    "ResNet is a deep convolutional neural network, uses residual connections to address vanishing gradients and performance degradation in deep networks.\n",
    "\n",
    "Simply put, the core idea of ResNet is a so called \"skip\" connection:\n",
    "\n",
    "\\begin{align}\\text{without skip}&: y=f(x) \\\\ \\text{with skip}&: y=f(x)+x\\end{align}\n",
    "\n",
    "A ResNet is typically consists of 3 parts: \n",
    "1. The first part uses one convolutional layer to project the input images into a feature space.\n",
    "2. The second part consists of multiple blocks for feature processing.\n",
    "3. The third part performs a spatial pooling and converts the feature into classification results.\n",
    "The building block of the second part is the key of ResNet, which is achieved by BasicBlock class.\n",
    "\n",
    "### Your task: \n",
    "Finish the BasicBlock class and perform training & evaluation using your ResNet. Compare how it performs with the previous vannila neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            BasicBlock(dim=32) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.linear = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.blocks(out)\n",
    "        out = F.avg_pool2d(out, 32)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "net = ResNet(n_blocks=6).cuda()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfoa4M5jPp-7"
   },
   "source": [
    "Then, it's time to train & test the performance of your neural network! "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 20 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].cuda(), data[1].cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC6CgPRXG6dF"
   },
   "source": [
    "\n",
    "# A Gentle Introduction to ``torch.autograd``\n",
    "\n",
    "``torch.autograd`` is PyTorch’s automatic differentiation engine that powers\n",
    "neural network training. In this section, you will get a basic sense of how ``autograd`` collects gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAfL8QFyIVix"
   },
   "source": [
    "In homework 8, you have encountered some matrix calculus equations. For example:\n",
    "\n",
    "\\begin{align}&\\frac{\\partial}{\\partial{X}}\\mathrm{\\text{Tr}}((Y-CX)^T(Y-CX))=-2 C^T (Y-CX)\\end{align}\n",
    "\n",
    "Now, let's see how we can perform this gradient calculation using pytorch's autograd mechanics (**without** the need of knowing the explicit formula of the gradient).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdGbJVerN9R3"
   },
   "source": [
    "We first create the matrix ``X``, ``C`` and ``Y``. Note that we pass ``requires_grad=True`` when creating matrix ``X``. This signals to ``autograd`` that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import torch\n",
    "X = torch.randn(4, 4, requires_grad=True)\n",
    "C = torch.randn(4, 4)\n",
    "Y = torch.randn(4, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "print('expected gradient on X:\\n', -2 * C.T @ (Y - C @ X))\n",
    "# calculate the result\n",
    "Q=torch.trace((Y-C @ X).T @ (Y-C.matmul(X)))\n",
    "# perform back-propagation\n",
    "Q.backward()\n",
    "print('autograd calculated gradient on X:\\n', X.grad)  # the gradients on X are stored in X.grad "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbC649zaQI3A"
   },
   "source": [
    "Nice! The gradient calculated by autograd is the same as our analytical solution!\n",
    "\n",
    "Now, let's re-execute the cell and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "print('expected gradient on X:\\n', -2 * C.T @ (Y - C @ X))\n",
    "# calculate the result\n",
    "Q=torch.trace((Y-C @ X).T @ (Y-C.matmul(X)))\n",
    "# perform back-propagation\n",
    "Q.backward()\n",
    "print('autograd calculated gradient on X:\\n', X.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJqUu7WXRNJI"
   },
   "source": [
    "Why in this time the autograd calculated gradient is not the same as our analytical result? This is because every time ``backward()`` function is called, gradients calculated by autograd are **accumulated** into ``X.grad``. \n",
    "\n",
    "As a result, if we are only interested in the gradients for current operations, we must first clear previously stored gradients:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "print('expected gradient on X:\\n', -2 * C.T @ (Y - C @ X))\n",
    "Q=torch.trace((Y-C @ X).T @ (Y-C.matmul(X)))\n",
    "X.grad.zero_()  # Important! Clear the previously stored gradients\n",
    "Q.backward()\n",
    "print('autograd calculated gradient on X:\\n', X.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdaO3qgJT1HC"
   },
   "source": [
    "Works as we expected again!\n",
    "\n",
    "Therefore, when performing neural network training, we will use ``zero_grad`` of optimizer to clear the gradient before performing gradient update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbQJsoy9gm0q"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
